{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\samge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Required Packages \n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import DutchStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import tiktoken\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import markdown\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "####\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samge\\AppData\\Local\\Temp\\ipykernel_5552\\1363499268.py:2: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"Reviews.csv\")\n"
     ]
    }
   ],
   "source": [
    "#CHANGE FILENAME\n",
    "data = pd.read_csv(\"Reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samge\\AppData\\Local\\Temp\\ipykernel_5552\\3318353539.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data['NPS_word_count'] = training_data['NPS driver explanation'].apply(word_count)\n"
     ]
    }
   ],
   "source": [
    "training_data = data.dropna(subset = [ \"NPS driver\", \"NPS driver explanation\"])\n",
    "def word_count(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "# Create a new column for word counts (optional, for inspection)\n",
    "training_data['NPS_word_count'] = training_data['NPS driver explanation'].apply(word_count)\n",
    "\n",
    "# Filter rows where word count is 5 or more\n",
    "training_data = training_data[training_data['NPS_word_count'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try: \n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "    \n",
    "training_data[\"Language\"] = training_data[\"Review general\"].apply(detect_language)\n",
    "\n",
    "\n",
    "training_english_reviews = training_data[training_data[\"Language\"] == \"en\"]\n",
    "training_dutch_reviews = training_data[(training_data[\"Language\"] == \"nl\")| (training_data[\"Language\"] == \"af\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#### English LANGUAGE PREPROCESSING ##\n",
    "######################################\n",
    "\n",
    "# Load the English spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_English(text):\n",
    "    \"\"\"\n",
    "    Cleans, normalizes, and applies lemmatization to English text.\n",
    "    \n",
    "    :param text: A string of text to preprocess.\n",
    "    :return: Preprocessed text as a string.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove URLs and any non-textual content\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
    "    \n",
    "    # Normalize text\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply lemmatization using spaCy\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Rejoin tokens into a single string\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samge\\AppData\\Local\\Temp\\ipykernel_5552\\3384719694.py:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\samge\\AppData\\Local\\Temp\\ipykernel_5552\\3076933442.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_english_reviews[\"P_Explanation\"] = training_english_reviews[\"NPS driver explanation\"].apply(preprocess_English)\n"
     ]
    }
   ],
   "source": [
    "training_english_reviews[\"P_Explanation\"] = training_english_reviews[\"NPS driver explanation\"].apply(preprocess_English)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DICTIONARY WITH COUNTS\n",
    "nps_driver_texts = training_english_reviews.groupby(\"NPS driver\")[\"P_Explanation\"].apply(lambda x: \" \".join(x)).to_dict()\n",
    "drivers = list(nps_driver_texts.keys())\n",
    "reviews = list(nps_driver_texts.values())\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(reviews)\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out(), index=drivers)\n",
    "likelihood_df = pd.DataFrame(columns=drivers)\n",
    "\n",
    "for word in vectorizer.get_feature_names_out():\n",
    "    driver_counts = dtm_df[word].tolist()  # Counts for each topic\n",
    "    smoothed = [x + 1 for x in driver_counts]  # Apply Laplace smoothing\n",
    "    likelihood_df.loc[word] = smoothed\n",
    "\n",
    "# Step 5: Normalize the word counts to calculate likelihoods\n",
    "for driver in drivers:\n",
    "    total_word_count = likelihood_df[driver].sum()\n",
    "    likelihood_df[driver] = likelihood_df[driver] / total_word_count\n",
    "\n",
    "# Step 6: Add words as a column and reset the index\n",
    "likelihood_df[\"words\"] = likelihood_df.index\n",
    "likelihood_df = likelihood_df.reset_index(drop=True)\n",
    "\n",
    "# Step 7: Extract the lexicon content\n",
    "lexicon = likelihood_df[\"words\"].astype(str).tolist()\n",
    "likelihood_df.to_csv(\"dict.csv\")\n",
    "\n",
    "lexicon = pd.DataFrame.from_dict(lexicon)\n",
    "lexicon.to_csv(\"Lexicon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_occurence = training_english_reviews[\"NPS driver\"].value_counts()\n",
    "prior_probabilities = driver_occurence/(len(training_english_reviews))\n",
    "prior_probabilities.to_csv(\"probs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
